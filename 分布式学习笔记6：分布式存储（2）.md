# 分布式学习笔记6：分布式存储（2）数据分区、HDFS

## 数据分区

数据分区能将数据存储和访问的负载分散到多个节点。合理分区的目标是将负载在各个节点均匀分布，避免出现偏斜（skew）和热点（hotspot），且分区需要兼顾跨区查询问题。

### 根据主键范围的分区

应用层遇到的绝大多数数据都可以找到一个唯一的标识来作为其主键

#### 均匀划分

假定我们有一个数据的取值范围是 0-N，有四个节点 N1~N4，那么就可以**【按主键范围均匀划分区间】**

![image-20200812141042860](img/image-20200812141042860.png)

优点：数据均匀划分，每个节点管理的数据相等，实现存储的负载均衡

缺点：没有针对【热点区间】进行优化，使存储热点数据的服务器承受过大压力

#### 非均匀划分

针对热点区间的情况，将一些不常用区间合并，将热点区间继续进行划分分到不同的服务器中，使【查询负载均衡】

![image-20200812141355181](img/image-20200812141355181.png)

但这种划分方式难以实现根据热点转移重新进行动态划分，且由于划分不均匀，还需要额外维护一个**全局索引表**，其保存 key 范围到存储节点的映射，进行查询时需要先查询该表

![image-20200812141531734](img/image-20200812141531734.png)

然而，这样的一个全局索引表同样也需要维护，维护该表的“主节点”就会成为单点失效节点

接下来介绍一种不需要全局索引表就能够解决数据热点问题的解决方案

#### 使用 Hash 进行分片

- Hash 函数的输入和输入的对应是随机的，但对于相同的输入则一定能给出相同的输出；
- Hash 值是难以碰撞的，假定函数会输出 n bit 的整数，如果我们任取两个值进行输入，则输出值相等的概率为 2<sup>-n</sup>

分片示例：将整个 hash 值空间均匀划分为与存储节点数量相等的空间，并据此将所有的数据在这些节点间进行均匀的分散存储

![image-20200812142646659](img/image-20200812142646659.png)

这样的分区是随机的，数据均匀分布。而由于数据的热点一般是连续的，这么一来热点区域也被分散存储了，实现均匀分布。

这种方式**无需全局索引表**：客户端可以自己通过数据的 key 值算出来该数据被存储的位置。

但这种方案的缺点也比较明显：会导致连续数据访问效率低下，且在增加或减少存储节点时会遇到困难——需要重新进行数据的移动。

##### 基于一致性 Hash 算法的分区

一致性 Hash 算法将整个 Hash 区间做成一个环，如下图中的 Hash 区间是 32 位，在做成一个环后

每个节点都有自己的唯一标识（比如 ip），我们对该标识取 hash，这样我们就能够将节点映射在该环上。理想状态下，如图的四个节点应该能够在 hash 环上均匀分布，这样一来，每两个节点之间就会有一个 hash 值区间了

![image-20200812144741943](img/image-20200812144741943.png)

这样一来，我们就可以人为规定：上游节点和下游节点（比如 node1 和 node2 ）之间的 hash 空间都由下游节点负责 （例如区间 1 就由 node2 负责）

当任意一个 key 输入后，我们就可以根据该 key 的 hash 值落在的空间来为其指定存储节点了

当我们增加新节点 node5 时（假定其落在 node2 和 node4 中间），就只需要将 node4 的部分数据传输给 node5 即可，对于其他节点则无任何影响

![image-20200812145208019](img/image-20200812145208019.png)

一些 p2p 存储系统也使用一致性 hash 算法——所有网络中的节点都被映射到 hash 环上，将大文件分块（使用 【文件名 + 块 id】 作为主键取 hash ），分块的 hash 落到哪个空间中则其存储就由相应的节点负责



##### MemCached

MemCached 是一个开源高性能分布式内存缓存服务器软件，为开发者提供了客户端库，可以方便地进行使用。其相比 Redis 性能更强但功能稍弱。

MemCached 使用 kv 存储模型，相当于将一个巨大的内存 HashMap 分布存储在多个 MemCached 节点上，常用于缓存数据库查询结果以减少数据库访问，实现对响应速度的提升。

（每次外部请求进行一次新的数据库查询，MemCached 会将查询结果缓存进入内存，对于同样的查询则直接从内存返回）

该 kv 存储的内容可以非常灵活，如果一个节点内存装满了，可以将该表使用 hash 切片存储到多个节点上。使用工具库开发的应用就可以在逻辑上认为自己有一个巨大的 kv 表即可，而无需关心分布式存储的内部原理。

![image-20200812150334162](img/image-20200812150334162.png)





## 分区与复制的组合使用

一个长度为 length 的大文件在 HDFS 中切片存储——按照字节偏移量每 128MB 进行均匀分区，最后一个分区若不满 128MB 则同样独立存储

![image-20200812152805398](img/image-20200812152805398.png)

对于该例子中大文件的第一段 P1 会以流水线的模式写入到 HDFS 中——首先直接写入 N1 节点，再由 N1 来向其他节点进行写入 N2 ，N2 再将 P1 写入 N3 。该分片由 N1 管理，N2、N3 都是 N1 中分片的备份

对于后续的其他文件分片，都要进行同样的操作。但该分片的管理可能由其他节点进行（而不全是 N1）

下图描述了这种分片和复制组合使用的示例：某个节点可以是某个分片的领导者（主库），自身同时又是其他领导者的追随者（从库）

![image-20200812153018547](img/image-20200812153018547.png)



## Ceph 简介

Ceph 是被广泛应用的开源分布式文件系统，已被集成进 Linux 内核，同时支持块存储、对象存储、文件存储。

对于块存储，其可以将多个物理硬盘虚拟为一个逻辑硬盘（比如整一个 1000TB 的），直接被挂载到虚拟机上作为一个大硬盘使用。



## 案例：分布式文件系统 HDFS

分布式文件系统将分布式系统中多个节点的存储资源整合在一起，向用户 / 应用程序呈现统一的存储空间和文件系统目录树，用户无需关心数据具体存储在哪个节点上。大文件则可以被自动分块并分别存储到不同的节点上。为提高可靠性，分布式文件系统还会进行多副本备份。

HDFS 采用主从构架，有一个主节点，即下面的 NameNode 服务器。

NameNode 服务器需要维护整个 FS 的元数据（文件名、文件长度（即以字节为单位的大小）、文件所有者、创建日期等）、管理其他的数据节点（ DataNode ，存储文件的实体数据 ）、进行多拷贝维护和管理存储的负载均衡

![image-20200811130942251](img/image-20200811130942251.png)

HDFS 中的所有文件都会被拆分为数据块，每个数据块大小的典型值是 128MB （但具体大小可以手动设置）。比如一个 300MB 的数据就会被拆分为三个数据块，分别为 128、128、44MB ，而每个数据块都会有一个全局唯一的编号 。

HDFS 会**在内存中维护两个表**（可以加快查询速度），这两个表在硬盘上的备份会以日志的形式进行（ HDFS 中的日志叫 Journal ）。HDFS 中还有一个专门的 backupNode 专职负责备份，同样会保存日志。

NameNode 对于元数据使用一种叫做 “ Checkpoint ” 的东西进行保存（实际上就是快照，定期进行整体的落盘备份）



#### 文件名-数据块对应表

![image-20200811131522979](img/image-20200811131522979.png)

上面的编号仅作示例，实际上可能是 hash 值、随机数这种全局唯一的值。

每个文件最后一块的大小要通过文件长度值进行计算（前面所有的块大小都是相等且已知的）

#### 数据块-物理节点对应表

以双备份策略为例，每个数据块都会在两个节点上存储两个副本

![image-20200811131833029](img/image-20200811131833029.png)



![image-20200811132049851](img/image-20200811132049851.png)

在实际的 HDFS 应用中，对文件的读请求要通过一个 Proxy 进行。过程如下：

1. 为 Proxy 提供读取指令及参数后（如：/user/bob/file2  offset=0  length=10），Proxy 会将读请求发送给 NameNode；
2. NameNode 通过查表确定该文件所在的数据块以及数据块对应的一组 DataNode 的 ip地址（上面的例子中，就是DN1、DN3 ），并将该位置返回给 Proxy；
3. 客户端根据某种规则来选择读取数据的节点（比如选一个延迟最小的），直接连接到该节点进行实际的数据读取。

客户端在从 NameNode 获取到元数据之后，可以将这些信息缓存下来。而且实际的数据传输都是 DataNode 和客户端直接进行的，与 NameNode 只会进行简单的交互，因此并不会对 NameNode 产生较大的压力





## HDFS 相关问题

该部分内容来自互联网

#### 客户端读取 HDFS 系统中指定文件偏移量处的数据时的工作流程？

1. 客户端调用 FileSystem.open() 打开要读取的文件；
2. DistributedFileSystem 进行 RPC 调用 namenode，确定文件起始块位置；
3. Client 对输入流调用 read()；
4. 存储文件起始块的 datanode 地址的 DFSInputStream 连接到距离最近的 datanode，对数据流反复调用 read()，可将数据从datanode 传输到 client；
5. 到达块末端时， DFSInputStream 关闭与该 datanode 的连接，寻找下一个最佳 datanode；
6. Client 读取数据时按照 DFSInputStream 与 datanode 新建连接的顺序进行读取，需要询问 namenode 来检索下一批需要的 datanode 的位置。读取完成后调用 FSDataInputStream .close()

#### 客户端向 HDFS 系统中指定文件追加写入数据的工作流程？

1. 打开已有的 HDFS 文件

   客户端调用 DistributedFileSystem.append() ，该函数会先调用 ClientProtocol.append() 获取文件最后一个数据块的位置信息，若文件最后一个数据块已写满则返回 null ，然后 append() 函数调用 DFSOutputStream.newStreamForAppend() 创建到这个数据块的 DFSOutputStream 输出流对象，获取文件租约，并将构造的 DFSOutputStream 函数包装为 HdfsDataOutputStream 对象并返回。

2. 建立数据流管道

   DFSOutputStream 的构造器会判断文件最后一个数据块是否已写满，若没有，则根据 ClientProtocol.append()  方法返回的该数据块的位置信息建立到该数据块的数据流管道。如果写满了则调用 ClientProtocol.addBlock() 向 namenode 申请一个新的空块后再建立数据流管道。

3. 通过数据流管道写入数据

   客户端通过该管道写入数据，与直接写 HDFS 文件类似。

4. 关闭输入流并提交文件

   客户端完成追加写操作后，调用 close() 关闭输出流，并调用 ClientProtocol.complete() 通知 namenode 提交这个文件中的所有数据块。

#### 新增数据块时，HDFS 如何选择存储该数据块的物理节点？

在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。这种策略减少了机架间的数据传输，这就提高了写操作的效率

#### HDFS 采用了哪些措施应对数据块损坏或丢失的问题？

HDFS 提供了数据完整性校验机制来保证数据的完整性。

当客户端创建 HDFS 文件时，它会计算文件的每个块的校验和 ，并将校验和存储在同一 HDFS 命名空间下的单独的隐藏文件中。

当客户端检索文件内容时，会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的校验和匹配。匹配失败则数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。

#### HDFS 应对主节点失效问题的措施？

DataNode 会通过心跳和 NameNode 保持通信，若 DataNode 超时未发送心跳， NameNode 就会认为这个 DataNode 已经失效，并立即查找这个 DataNode上存储的数据块有哪些，以及这些数据块还存储在哪些服务器上，随后通知这些服务器再复制一份数据块到其他服务器上，保证HDFS存储的数据块备份数符合用户设置的数目，即使再出现服务器宕机，也不会丢失数据。

#### NodeName 维护的“数据块-物理节点对应表”是否需要在硬盘中备份？为什么？

不需要。

因 Editlog 和 FsImage 保存在了本地的文件系统中，Namenode 启动时，它从硬盘中读取这两个文件，将所有 Editlog 中的事务作用在内存中的 FsImage 上，并将这个新版本的 FsImage 从内存中保存到本地磁盘上，然后删除旧的 Editlog ，因为这个旧的 Editlog 的事务都已经作用在FsImage上了。这个过程称为一个检查点(checkpoint)。在当前实现中， 检查点只发生在 Namenode 启动时。 但是 Namenode 可以配置成支持维护多个 FsImage 和 Editlog的副本。任何对 FsImage 或者 Editlog 的修改，都将同步到它们的副本上。